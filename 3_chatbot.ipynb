{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guptavishu1000/langgraph-crash-course/blob/main/3_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph\n",
        "!pip install -U langchain-google-genai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LzeVWt48_0Jr",
        "outputId": "767c3e05-13df-44f7-89cf-b7a67037994d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "LzeVWt48_0Jr",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.7-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core>=0.1 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.75)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.8-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (0.4.27)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.1->langgraph) (25.0)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.1->langgraph) (0.24.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core>=0.1->langgraph) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n",
            "Downloading langgraph-0.6.7-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.8-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.7/216.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.6.7 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.8 ormsgpack-1.10.0\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.3.75)\n",
            "Collecting google-ai-generativelanguage<1,>=0.7 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Collecting filetype<2,>=1.2 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (0.4.27)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3.75->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.24.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.12-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.7.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.7.0 langchain-google-genai-2.1.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "2177840dc4ed44fc855b4c3b4e1353b4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-06-20T21:25:36.850285Z",
          "start_time": "2025-06-20T21:25:36.838246Z"
        },
        "id": "initial_id"
      },
      "source": [
        "from typing import Annotated\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "from langchain.chat_models import init_chat_model\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Load the secret key into an environment variable\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "rNpn3D6GCNmo"
      },
      "id": "rNpn3D6GCNmo",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-20T21:25:40.029147Z",
          "start_time": "2025-06-20T21:25:40.024148Z"
        },
        "id": "3c96164fc8cf8136"
      },
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model"
      ],
      "id": "3c96164fc8cf8136",
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "help(init_chat_model)"
      ],
      "metadata": {
        "id": "fNw2_nLYCRFg",
        "outputId": "eb66d777-88e6-4f06-e550-60f81edc5017",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fNw2_nLYCRFg",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function init_chat_model in module langchain.chat_models.base:\n",
            "\n",
            "init_chat_model(model: 'Optional[str]' = None, *, model_provider: 'Optional[str]' = None, configurable_fields: \"Optional[Union[Literal['any'], list[str], tuple[str, ...]]]\" = None, config_prefix: 'Optional[str]' = None, **kwargs: 'Any') -> 'Union[BaseChatModel, _ConfigurableModel]'\n",
            "    Initialize a ChatModel in a single line using the model's name and provider.\n",
            "\n",
            "    .. note::\n",
            "        Must have the integration package corresponding to the model provider installed.\n",
            "        You should look at the `provider integration's API reference <https://python.langchain.com/api_reference/reference.html#integrations>`__\n",
            "        to see what parameters are supported by the model.\n",
            "\n",
            "    Args:\n",
            "        model: The name of the model, e.g. ``'o3-mini'``, ``'claude-3-5-sonnet-latest'``. You can\n",
            "            also specify model and model provider in a single argument using\n",
            "            ``'{model_provider}:{model}'`` format, e.g. ``'openai:o1'``.\n",
            "        model_provider: The model provider if not specified as part of model arg (see\n",
            "            above). Supported model_provider values and the corresponding integration\n",
            "            package are:\n",
            "\n",
            "            - ``openai``              -> ``langchain-openai``\n",
            "            - ``anthropic``           -> ``langchain-anthropic``\n",
            "            - ``azure_openai``        -> ``langchain-openai``\n",
            "            - ``azure_ai``            -> ``langchain-azure-ai``\n",
            "            - ``google_vertexai``     -> ``langchain-google-vertexai``\n",
            "            - ``google_genai``        -> ``langchain-google-genai``\n",
            "            - ``bedrock``             -> ``langchain-aws``\n",
            "            - ``bedrock_converse``    -> ``langchain-aws``\n",
            "            - ``cohere``              -> ``langchain-cohere``\n",
            "            - ``fireworks``           -> ``langchain-fireworks``\n",
            "            - ``together``            -> ``langchain-together``\n",
            "            - ``mistralai``           -> ``langchain-mistralai``\n",
            "            - ``huggingface``         -> ``langchain-huggingface``\n",
            "            - ``groq``                -> ``langchain-groq``\n",
            "            - ``ollama``              -> ``langchain-ollama``\n",
            "            - ``google_anthropic_vertex``    -> ``langchain-google-vertexai``\n",
            "            - ``deepseek``            -> ``langchain-deepseek``\n",
            "            - ``ibm``                 -> ``langchain-ibm``\n",
            "            - ``nvidia``              -> ``langchain-nvidia-ai-endpoints``\n",
            "            - ``xai``                 -> ``langchain-xai``\n",
            "            - ``perplexity``          -> ``langchain-perplexity``\n",
            "\n",
            "            Will attempt to infer model_provider from model if not specified. The\n",
            "            following providers will be inferred based on these model prefixes:\n",
            "\n",
            "            - ``gpt-3...`` | ``gpt-4...`` | ``o1...`` -> ``openai``\n",
            "            - ``claude...``                       -> ``anthropic``\n",
            "            - ``amazon...``                       -> ``bedrock``\n",
            "            - ``gemini...``                       -> ``google_vertexai``\n",
            "            - ``command...``                      -> ``cohere``\n",
            "            - ``accounts/fireworks...``           -> ``fireworks``\n",
            "            - ``mistral...``                      -> ``mistralai``\n",
            "            - ``deepseek...``                     -> ``deepseek``\n",
            "            - ``grok...``                         -> ``xai``\n",
            "            - ``sonar...``                        -> ``perplexity``\n",
            "        configurable_fields: Which model parameters are configurable:\n",
            "\n",
            "            - None: No configurable fields.\n",
            "            - ``'any'``: All fields are configurable. **See Security Note below.**\n",
            "            - Union[List[str], Tuple[str, ...]]: Specified fields are configurable.\n",
            "\n",
            "            Fields are assumed to have config_prefix stripped if there is a\n",
            "            config_prefix. If model is specified, then defaults to None. If model is\n",
            "            not specified, then defaults to ``(\"model\", \"model_provider\")``.\n",
            "\n",
            "            ***Security Note***: Setting ``configurable_fields=\"any\"`` means fields like\n",
            "            ``api_key``, ``base_url``, etc. can be altered at runtime, potentially redirecting\n",
            "            model requests to a different service/user. Make sure that if you're\n",
            "            accepting untrusted configurations that you enumerate the\n",
            "            ``configurable_fields=(...)`` explicitly.\n",
            "\n",
            "        config_prefix: If ``'config_prefix'`` is a non-empty string then model will be\n",
            "            configurable at runtime via the\n",
            "            ``config[\"configurable\"][\"{config_prefix}_{param}\"]`` keys. If\n",
            "            ``'config_prefix'`` is an empty string then model will be configurable via\n",
            "            ``config[\"configurable\"][\"{param}\"]``.\n",
            "        temperature: Model temperature.\n",
            "        max_tokens: Max output tokens.\n",
            "        timeout: The maximum time (in seconds) to wait for a response from the model\n",
            "            before canceling the request.\n",
            "        max_retries: The maximum number of attempts the system will make to resend a\n",
            "            request if it fails due to issues like network timeouts or rate limits.\n",
            "        base_url: The URL of the API endpoint where requests are sent.\n",
            "        rate_limiter: A ``BaseRateLimiter`` to space out requests to avoid exceeding\n",
            "            rate limits.\n",
            "        kwargs: Additional model-specific keyword args to pass to\n",
            "            ``<<selected ChatModel>>.__init__(model=model_name, **kwargs)``.\n",
            "\n",
            "    Returns:\n",
            "        A BaseChatModel corresponding to the model_name and model_provider specified if\n",
            "        configurability is inferred to be False. If configurable, a chat model emulator\n",
            "        that initializes the underlying model at runtime once a config is passed in.\n",
            "\n",
            "    Raises:\n",
            "        ValueError: If model_provider cannot be inferred or isn't supported.\n",
            "        ImportError: If the model provider integration package is not installed.\n",
            "\n",
            "    .. dropdown:: Init non-configurable model\n",
            "        :open:\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            # pip install langchain langchain-openai langchain-anthropic langchain-google-vertexai\n",
            "            from langchain.chat_models import init_chat_model\n",
            "\n",
            "            o3_mini = init_chat_model(\"openai:o3-mini\", temperature=0)\n",
            "            claude_sonnet = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0)\n",
            "            gemini_2_flash = init_chat_model(\"google_vertexai:gemini-2.0-flash\", temperature=0)\n",
            "\n",
            "            o3_mini.invoke(\"what's your name\")\n",
            "            claude_sonnet.invoke(\"what's your name\")\n",
            "            gemini_2_flash.invoke(\"what's your name\")\n",
            "\n",
            "\n",
            "    .. dropdown:: Partially configurable model with no default\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            # pip install langchain langchain-openai langchain-anthropic\n",
            "            from langchain.chat_models import init_chat_model\n",
            "\n",
            "            # We don't need to specify configurable=True if a model isn't specified.\n",
            "            configurable_model = init_chat_model(temperature=0)\n",
            "\n",
            "            configurable_model.invoke(\n",
            "                \"what's your name\",\n",
            "                config={\"configurable\": {\"model\": \"gpt-4o\"}}\n",
            "            )\n",
            "            # GPT-4o response\n",
            "\n",
            "            configurable_model.invoke(\n",
            "                \"what's your name\",\n",
            "                config={\"configurable\": {\"model\": \"claude-3-5-sonnet-latest\"}}\n",
            "            )\n",
            "            # claude-3.5 sonnet response\n",
            "\n",
            "    .. dropdown:: Fully configurable model with a default\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            # pip install langchain langchain-openai langchain-anthropic\n",
            "            from langchain.chat_models import init_chat_model\n",
            "\n",
            "            configurable_model_with_default = init_chat_model(\n",
            "                \"openai:gpt-4o\",\n",
            "                configurable_fields=\"any\",  # this allows us to configure other params like temperature, max_tokens, etc at runtime.\n",
            "                config_prefix=\"foo\",\n",
            "                temperature=0\n",
            "            )\n",
            "\n",
            "            configurable_model_with_default.invoke(\"what's your name\")\n",
            "            # GPT-4o response with temperature 0\n",
            "\n",
            "            configurable_model_with_default.invoke(\n",
            "                \"what's your name\",\n",
            "                config={\n",
            "                    \"configurable\": {\n",
            "                        \"foo_model\": \"anthropic:claude-3-5-sonnet-20240620\",\n",
            "                        \"foo_temperature\": 0.6\n",
            "                    }\n",
            "                }\n",
            "            )\n",
            "            # Claude-3.5 sonnet response with temperature 0.6\n",
            "\n",
            "    .. dropdown:: Bind tools to a configurable model\n",
            "\n",
            "        You can call any ChatModel declarative methods on a configurable model in the\n",
            "        same way that you would with a normal model.\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            # pip install langchain langchain-openai langchain-anthropic\n",
            "            from langchain.chat_models import init_chat_model\n",
            "            from pydantic import BaseModel, Field\n",
            "\n",
            "            class GetWeather(BaseModel):\n",
            "                '''Get the current weather in a given location'''\n",
            "\n",
            "                location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
            "\n",
            "            class GetPopulation(BaseModel):\n",
            "                '''Get the current population in a given location'''\n",
            "\n",
            "                location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
            "\n",
            "            configurable_model = init_chat_model(\n",
            "                \"gpt-4o\",\n",
            "                configurable_fields=(\"model\", \"model_provider\"),\n",
            "                temperature=0\n",
            "            )\n",
            "\n",
            "            configurable_model_with_tools = configurable_model.bind_tools([GetWeather, GetPopulation])\n",
            "            configurable_model_with_tools.invoke(\n",
            "                \"Which city is hotter today and which is bigger: LA or NY?\"\n",
            "            )\n",
            "            # GPT-4o response with tool calls\n",
            "\n",
            "            configurable_model_with_tools.invoke(\n",
            "                \"Which city is hotter today and which is bigger: LA or NY?\",\n",
            "                config={\"configurable\": {\"model\": \"claude-3-5-sonnet-20240620\"}}\n",
            "            )\n",
            "            # Claude-3.5 sonnet response with tools\n",
            "\n",
            "    .. versionadded:: 0.2.7\n",
            "\n",
            "    .. versionchanged:: 0.2.8\n",
            "\n",
            "        Support for ``configurable_fields`` and ``config_prefix`` added.\n",
            "\n",
            "    .. versionchanged:: 0.2.12\n",
            "\n",
            "        Support for Ollama via langchain-ollama package added\n",
            "        (langchain_ollama.ChatOllama). Previously,\n",
            "        the now-deprecated langchain-community version of Ollama was imported\n",
            "        (langchain_community.chat_models.ChatOllama).\n",
            "\n",
            "        Support for AWS Bedrock models via the Converse API added\n",
            "        (model_provider=\"bedrock_converse\").\n",
            "\n",
            "    .. versionchanged:: 0.3.5\n",
            "\n",
            "        Out of beta.\n",
            "\n",
            "    .. versionchanged:: 0.3.19\n",
            "\n",
            "        Support for Deepseek, IBM, Nvidia, and xAI models added.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n",
        "llm.invoke([{\"role\": \"user\", \"content\": \"Who walked on the moon for the first time?\"}])"
      ],
      "metadata": {
        "id": "OMdvueOtB_PG",
        "outputId": "8850d9e7-3677-4fdc-ce35-e855a07b8c2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OMdvueOtB_PG",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Neil Armstrong was the first person to walk on the moon.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--4eebefc8-ad23-4e02-a71b-1853a228e158-0', usage_metadata={'input_tokens': 10, 'output_tokens': 13, 'total_tokens': 23, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-20T21:25:41.406741Z",
          "start_time": "2025-06-20T21:25:41.386238Z"
        },
        "id": "efdc5e9a8ab40001"
      },
      "cell_type": "code",
      "source": [
        "llm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "def chatbot(state: State) -> State:\n",
        "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
        "\n",
        "builder = StateGraph(State)\n",
        "builder.add_node(\"chatbot_node\", chatbot)\n",
        "\n",
        "builder.add_edge(START, \"chatbot_node\")\n",
        "builder.add_edge(\"chatbot_node\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "id": "efdc5e9a8ab40001",
      "outputs": [],
      "execution_count": 12
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-20T22:04:23.936094Z",
          "start_time": "2025-06-20T22:04:23.575200Z"
        },
        "id": "3d157a437a401e2a",
        "outputId": "22b47712-7e3b-4f7f-8f02-ba460508625a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "message = {\"role\": \"user\", \"content\": \"Who walked on the moon for the first time? Print only the name\"}\n",
        "# message = {\"role\": \"user\", \"content\": \"What is the latest price of MSFT stock?\"}\n",
        "response = graph.invoke({\"messages\":[message]})\n",
        "\n",
        "response[\"messages\"]"
      ],
      "id": "3d157a437a401e2a",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Who walked on the moon for the first time? Print only the name', additional_kwargs={}, response_metadata={}, id='65c617d7-4ad9-44f7-8236-3b7e40623153'),\n",
              " AIMessage(content='Neil Armstrong', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--16d28ea8-c6aa-4fb8-b38a-d2ca0fcfd0c8-0', usage_metadata={'input_tokens': 14, 'output_tokens': 3, 'total_tokens': 17, 'input_token_details': {'cache_read': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "execution_count": 13
    },
    {
      "metadata": {
        "id": "d7e0e36652496d92",
        "outputId": "afb4a638-604b-42ee-9dbf-91a52c7a2d13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "state = None\n",
        "while True:\n",
        "    in_message = input(\"You: \")\n",
        "    if in_message.lower() in {\"quit\",\"exit\"}:\n",
        "        break\n",
        "    if state is None:\n",
        "        state: State = {\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": in_message}]\n",
        "        }\n",
        "    else:\n",
        "        state[\"messages\"].append({\"role\": \"user\", \"content\": in_message})\n",
        "\n",
        "    state = graph.invoke(state)\n",
        "    print(\"Bot:\", state[\"messages\"][-1].content)"
      ],
      "id": "d7e0e36652496d92",
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: who is the 7th prime minister of india and what do you mean by Statistics\n",
            "Bot: The 7th Prime Minister of India was **Vishwanath Pratap Singh**, often referred to as **V. P. Singh**. He served from December 2, 1989, to November 10, 1990.\n",
            "\n",
            "Now, let's talk about **Statistics**:\n",
            "\n",
            "Statistics is a branch of mathematics that deals with the collection, analysis, interpretation, presentation, and organization of data. It's a powerful tool used to understand and make inferences from data in various fields.\n",
            "\n",
            "Here's a breakdown of what that means:\n",
            "\n",
            "*   **Collection:** Gathering data from various sources, such as surveys, experiments, observations, or existing databases.\n",
            "\n",
            "*   **Organization:** Arranging data in a meaningful way, often through tables, charts, or graphs, to make it easier to understand and analyze.\n",
            "\n",
            "*   **Presentation:** Displaying data visually or in written form, highlighting key patterns and trends.\n",
            "\n",
            "*   **Analysis:** Applying mathematical and computational techniques to examine data, identify relationships, and draw conclusions. This can involve calculating averages, finding correlations, testing hypotheses, and building models.\n",
            "\n",
            "*   **Interpretation:** Making sense of the results of the analysis and drawing meaningful conclusions that can be applied to real-world situations.\n",
            "\n",
            "**In simpler terms, statistics helps us:**\n",
            "\n",
            "*   **Summarize large amounts of data:** Imagine trying to understand the opinions of millions of people without any way to organize and analyze their responses. Statistics provides the tools to do just that.\n",
            "\n",
            "*   **Identify patterns and trends:** By analyzing data, we can uncover hidden relationships and predict future outcomes.\n",
            "\n",
            "*   **Make informed decisions:** Whether it's a business deciding on a marketing strategy or a government formulating public policy, statistics provides the evidence needed to make sound choices.\n",
            "\n",
            "*   **Test hypotheses:** We can use statistics to determine whether our assumptions about the world are supported by evidence.\n",
            "\n",
            "**Examples of how statistics is used in the real world:**\n",
            "\n",
            "*   **Healthcare:** Analyzing clinical trial data to determine the effectiveness of new drugs.\n",
            "*   **Business:** Forecasting sales, understanding customer behavior, and optimizing marketing campaigns.\n",
            "*   **Economics:** Tracking economic indicators, analyzing market trends, and predicting recessions.\n",
            "*   **Social Sciences:** Studying social trends, conducting surveys, and evaluating the impact of social programs.\n",
            "*   **Sports:** Analyzing player performance, predicting game outcomes, and developing training strategies.\n",
            "*   **Environmental Science:** Monitoring pollution levels, studying climate change, and assessing the impact of human activities on the environment.\n",
            "\n",
            "Essentially, statistics is about turning data into knowledge and using that knowledge to make better decisions.\n",
            "You: quit\n"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kbjKhL3hGRSY"
      },
      "id": "kbjKhL3hGRSY",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}